{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "This workshop will explain some extended reasoning behind information entropy and Kullback-Leibler divergence. In short, ability to accurately predict symbols allows for more efficient compression of information. This efficiency of compression is often used as a proxy to predictive model quality if the form of LogLikelihood loss.\n",
    "\n",
    "##### Acknowledgements\n",
    "The RNN parts were heavily inspired by [karpathy/char-rnn](https://github.com/karpathy/char-rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first what is Kullback-Leibler divergence, and what is entropy.\n",
    "\n",
    "### Kullback-Leibler divergence\n",
    "\n",
    "$D_{KL}(P || Q) = \\sum_{x\\in X} P(x)  log \\frac{P(x)}{Q(x)}$\n",
    "\n",
    "This can be viewed as expectation over log of ratio $P$ and $Q$ . Do note that things get weird if $P(x)$ or $Q(x)$ could be zero, so just pretend that its $\\epsilon > 0$ in that case. There will be some explanations for such cases later.\n",
    "\n",
    "So, $E_{P(x)} [\\log \\frac{P(x)}{Q(x)}]$, for some symbol $x$ that is sampled from distribution $P$, what would be the extra amount of bits needed to encode it if we created our encoding table for distribtution $Q$.\n",
    "Ops, bits, encoding table, we did not say anything about that yet, so, lets put a bookmark here $Bookmark 1.$ and we will continue this after we dealt with encoding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding\n",
    "\n",
    "Encoding is all about replacing $x\\in X$ with sequences $y_1,y_2,\\dots,y_k$ where $y_i\\in Y$, such that those sequences can be transformed back into original symbols.\n",
    "\n",
    "Specifically, what we care about most is replacement with binary sequences $\\{1, 0\\}_k$ i.e. bits. The end goal is to use as little bits as needed to encode a sequence written in symbols being encoded. So as not to get entagled in theory too much, we will jump straight to a method of getting such codes, [Huffman coding](https://en.wikipedia.org/wiki/Huffman_coding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def huffman(symbol_freq_pairs):\n",
    "    heap = [[freq, (sym, freq, [])] for sym, freq in symbol_freq_pairs]\n",
    "    heapq.heapify(heap)\n",
    "    while len(heap) > 1:\n",
    "        a = heapq.heappop(heap)\n",
    "        b = heapq.heappop(heap)\n",
    "        for sym, freq, code in a[1:]:\n",
    "            code.append('0')\n",
    "        for sym, freq, code in b[1:]:\n",
    "            code.append('1')\n",
    "        heapq.heappush(heap, [a[0] + b[0]] + a[1:] + b[1:])\n",
    "    return {sym:(freq, ''.join(code[::-1])) for sym, freq, code in heapq.heappop(heap)[1:]}\n",
    "\n",
    "def pretty(coding_table, sort_key=lambda x: (len(x[1][1]), x[0])):\n",
    "    for sym, (freq, code) in sorted(coding_table.items(), key=sort_key):\n",
    "        print(\"{:3} {:5.4f} {:10} {}\".format(sym, freq, code, len(code)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see an example for this, first uniform distribution, then for a random distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codes for uniform distribution.\n",
      "u   0.0385 0000       4\n",
      "v   0.0385 0001       4\n",
      "w   0.0385 0010       4\n",
      "x   0.0385 0011       4\n",
      "y   0.0385 0100       4\n",
      "z   0.0385 0101       4\n",
      "a   0.0385 01100      5\n",
      "b   0.0385 01101      5\n",
      "c   0.0385 01110      5\n",
      "d   0.0385 01111      5\n",
      "e   0.0385 10000      5\n",
      "f   0.0385 10001      5\n",
      "g   0.0385 10010      5\n",
      "h   0.0385 10011      5\n",
      "i   0.0385 10100      5\n",
      "j   0.0385 10101      5\n",
      "k   0.0385 10110      5\n",
      "l   0.0385 10111      5\n",
      "m   0.0385 11000      5\n",
      "n   0.0385 11001      5\n",
      "o   0.0385 11010      5\n",
      "p   0.0385 11011      5\n",
      "q   0.0385 11100      5\n",
      "r   0.0385 11101      5\n",
      "s   0.0385 11110      5\n",
      "t   0.0385 11111      5\n",
      "Codes for random distribution.\n",
      "u   0.1000 001        3\n",
      "g   0.0800 1100       4\n",
      "k   0.0600 0101       4\n",
      "l   0.0800 1110       4\n",
      "o   0.0600 0110       4\n",
      "s   0.0600 1000       4\n",
      "w   0.0400 0000       4\n",
      "x   0.0600 1001       4\n",
      "y   0.0400 0001       4\n",
      "z   0.0400 0100       4\n",
      "b   0.0400 01111      5\n",
      "c   0.0400 10100      5\n",
      "h   0.0400 10111      5\n",
      "r   0.0200 01110      5\n",
      "t   0.0400 11110      5\n",
      "v   0.0400 11111      5\n",
      "d   0.0200 101010     6\n",
      "e   0.0200 101011     6\n",
      "f   0.0200 101100     6\n",
      "i   0.0200 101101     6\n",
      "j   0.0200 110100     6\n",
      "p   0.0200 110110     6\n",
      "q   0.0200 110111     6\n",
      "a   0.0200 1101011    7\n",
      "m   0.0000 11010100   8\n",
      "n   0.0000 11010101   8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import Counter\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "symbols = [chr(i) for i in range(ord('a'), ord('z') + 1)]\n",
    "uniform = zip(symbols, np.full(len(symbols), 1.0 / len(symbols)))\n",
    "print(\"Codes for uniform distribution.\")\n",
    "pretty(huffman(uniform))\n",
    "\n",
    "N = 50\n",
    "random_count = Counter(np.random.choice(symbols, N))\n",
    "random = zip(symbols, [random_count[sym] / N for sym in symbols])\n",
    "print(\"Codes for random distribution.\")\n",
    "pretty(huffman(random))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Huffman coding operates per-symbol basis, so there is some inefficiencies (i.e. if there are only 2 symbols to begin with, they will be replaced with 1 and 0 regardless of their frequencies). It behaves optimally if all frequencies are powers of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a   0.2500 00         2\n",
      "b   0.1250 010        3\n",
      "c   0.0625 0110       4\n",
      "d   0.5000 1          1\n",
      "e   0.0625 0111       4\n",
      "Note that:\n",
      "-log2(0.25) = 2.0\n",
      "-log2(0.125) = 3.0\n",
      "-log2(0.0625) = 4.0\n",
      "-log2(0.5) = 1.0\n",
      "-log2(0.0625) = 4.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "powers2 = [('a', 1/4),\n",
    "           ('b', 1/8),\n",
    "           ('c', 1/16),\n",
    "           ('d', 1/2),\n",
    "           ('e', 1/16)\n",
    "          ]\n",
    "pretty(huffman(powers2), sort_key=lambda x: x[0])\n",
    "\n",
    "print(\"Note that:\")\n",
    "for f in [x[1] for x in powers2]:\n",
    "    print(\"-log2({}) = {}\".format(f, -math.log2(f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The theoretical minimum of bits-per-symbol coding is $-log_2(symbol\\_frequency)$ if the expected number of bits per symbol wants to be minimized $E_{P(x)} [- log_2 P(x)]$. (Proof omitted)\n",
    "\n",
    "This expectation in extended form is $- \\sum_{x\\in X} P(x) log_2 P(x)$ (hey, that looks like entropy)\n",
    "\n",
    "But what if we make the coding using the \"wrong\" wrong distribution, say $Q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good code\n",
      "a   0.2500 00         2\n",
      "b   0.1250 010        3\n",
      "c   0.0625 0110       4\n",
      "d   0.5000 1          1\n",
      "e   0.0625 0111       4\n",
      "Wrong code\n",
      "a   0.0625 0000       4\n",
      "b   0.2500 01         2\n",
      "c   0.5000 1          1\n",
      "d   0.0625 0001       4\n",
      "e   0.1250 001        3\n",
      "Note that:\n",
      "log2(0.25 / 0.0625) = 2.0\n",
      "log2(0.125 / 0.25) = -1.0\n",
      "log2(0.0625 / 0.5) = -3.0\n",
      "log2(0.5 / 0.0625) = 3.0\n",
      "log2(0.0625 / 0.125) = -1.0\n"
     ]
    }
   ],
   "source": [
    "wrong_powers2 = [('a', 1/16),\n",
    "                 ('b', 1/4),\n",
    "                 ('c', 1/2),\n",
    "                 ('d', 1/16),\n",
    "                 ('e', 1/8)\n",
    "                ]\n",
    "print(\"Good code\")\n",
    "pretty(huffman(powers2), sort_key=lambda x: x[0])\n",
    "print(\"Wrong code\")\n",
    "pretty(huffman(wrong_powers2), sort_key=lambda x: x[0])\n",
    "\n",
    "print(\"Note that:\")\n",
    "for f1, f2 in zip([x[1] for x in powers2], [x[1] for x in wrong_powers2]):\n",
    "    print(\"log2({} / {}) = {}\".format(f1, f2, math.log2(f1/f2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative $log_2$ of symbol probability ratio is the difference in number of bits needed to represent those symbols, and the expectation of that over $P(x)$ is the expected number of extra bits $E_{P(x)} [\\log \\frac{P(x)}{Q(x)}]$ (hey this was $Bookmark1$ we are back on track).\n",
    "\n",
    "Also, $log_2 \\frac{P(x)}{Q(x)} = - log_2 Q(x) - (- log_2P(x))$, so there that difference becomes obvious.\n",
    "\n",
    "But lets get back to $D_{KL}(P || Q) = \\sum_{x\\in X} P(x)  log \\frac{P(x)}{Q(x)}$.\n",
    "\n",
    "We can split that to $\\sum_{x\\in X} P(x)  log \\frac{P(x)}{Q(x)} = -\\sum_{x\\in X} P(x) log_2 Q(x) - (- \\sum_{x\\in X} P(x) log_2 P(x))$, which is the difference between expected bits per symbol when using coding table constructed for $Q(x)$ and when using one constructed for $P(x)$ which would be the optimal code. The second component is the _Shannon entropy_ and is the minimum of bits needed to encode data under distribution $P(x)$.\n",
    "\n",
    "Hey, $-\\sum_{x\\in X} P(x) log_2 Q(x)$ is cross-entropy and also that negative LogLikelihood we love to optimize!\n",
    "\n",
    "A very, very important thing to understand is: _What is $P(x)$?_ But for that, lets get some data first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shakespearian datasets\n",
    "\n",
    "We are gonna grab Shakespeares collected works from https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "if not os.path.isfile('shakespeare.txt'):\n",
    "    shakespeare_by_karpathy = 'https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt'\n",
    "    urllib.request.urlretrieve(shakespeare_by_karpathy, 'shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_raw = open('shakespeare.txt', 'r').readlines()\n",
    "shakespeare = []\n",
    "shake_it = iter(shakespeare_raw)\n",
    "speaker = next(shake_it).rstrip()[:-1]\n",
    "lines = []\n",
    "while True:\n",
    "    try:\n",
    "        line = next(shake_it).rstrip()\n",
    "        if line:\n",
    "            lines.append(line)\n",
    "        else:\n",
    "            shakespeare.append(' '.join(lines))\n",
    "            lines = []\n",
    "            speaker = next(shake_it).rstrip()[:-1]\n",
    "    except StopIteration:\n",
    "        shakespeare.append(''.join(lines))\n",
    "        del lines\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31023,\n",
       " ['Before we proceed any further, hear me speak.',\n",
       "  'Speak, speak.',\n",
       "  'You are all resolved rather to die than to famish?',\n",
       "  'Resolved. resolved.',\n",
       "  'First, you know Caius Marcius is chief enemy to the people.',\n",
       "  \"We know't, we know't.\",\n",
       "  \"Let us kill him, and we'll have corn at our own price. Is't a verdict?\",\n",
       "  \"No more talking on't; let it be done: away, away!\",\n",
       "  'One word, good citizens.',\n",
       "  'We are accounted poor citizens, the patricians good. What authority surfeits on would relieve us: if they would yield us but the superfluity, while it were wholesome, we might guess they relieved us humanely; but they think we are too dear: the leanness that afflicts us, the object of our misery, is as an inventory to particularise their abundance; our sufferance is a gain to them Let us revenge this with our pikes, ere we become rakes: for the gods know I speak this in hunger for bread, not in thirst for revenge.'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shakespeare), shakespeare[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, our samples are lines by speakers, sort of. I will make an important distiction now, we shall have 2 datasets, one will be _The Complete Shakespeare_ noted with $P_{CS}$ and _Shakespeare Lost Works_ noted with $P_{LW}$.\n",
    "\n",
    "The difference is that $P_{CS}$ is fully known, since it is defined by the data above, $P_{LW}$ implies some lost works by Shakespeare, so the dataset above is just a sample from the full distribution.\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29,\n",
       " [\"Let us kill him, and we'll have corn at our own price. Is't a verdict?\",\n",
       "  'Let us take the law of our sides; let them begin.',\n",
       "  'Let us entreat you stay till after dinner.',\n",
       "  \"Let us to the great supper: their cheer is the greater that I am subdued. Would the cook were of my mind! Shall we go prove what's to be done?\",\n",
       "  \"Let us withdraw; 'twill be a storm.\",\n",
       "  'Let us deal justly. Sleepest or wakest thou, jolly shepherd? Thy sheep be in the corn; And for one blast of thy minikin mouth, Thy sheep shall take no harm. Pur! the cat is gray.',\n",
       "  \"Let us go. Come; Our separation so abides, and flies, That thou, residing here, go'st yet with me, And I, hence fleeting, here remain with thee. Away!\",\n",
       "  'Let us go. Good Enobarbus, make yourself my guest Whilst you abide here.',\n",
       "  \"Let us score their backs, And snatch 'em up, as we take hares, behind: 'Tis sport to maul a runner.\",\n",
       "  'Let us sit and mock the good housewife Fortune from her wheel, that her gifts may henceforth be bestowed equally.',\n",
       "  'Let us not leave him out.',\n",
       "  \"Let us do so: for we are at the stake, And bay'd about with many enemies; And some that smile have in their hearts, I fear, Millions of mischiefs.\",\n",
       "  'Let us go see your son, I pray you: I long to talk with the young noble soldier.',\n",
       "  \"Let us from point to point this story know, To make the even truth in pleasure flow. If thou be'st yet a fresh uncropped flower, Choose thou thy husband, and I'll pay thy dower; For I can guess that by thy honest aid Thou keep'st a wife herself, thyself a maid. Of that and all the progress, more or less, Resolvedly more leisure shall express: All yet seems well; and if it end so meet, The bitter past, more welcome is the sweet.\",\n",
       "  'Let us make ready straight.',\n",
       "  'Let us seek out some desolate shade, and there Weep our sad bosoms empty.',\n",
       "  \"Let us rather Hold fast the mortal sword, and like good men Bestride our down-fall'n birthdom: each new morn New widows howl, new orphans cry, new sorrows Strike heaven on the face, that it resounds As if it felt with Scotland and yell'd out Like syllable of dolour.\",\n",
       "  'Let us hear, sweet Bottom.',\n",
       "  'Let us confess and turn it to a jest.',\n",
       "  'Let us haste to hear it, And call the noblest to the audience. For me, with sorrow I embrace my fortune: I have some rights of memory in this kingdom, Which now to claim my vantage doth invite me.',\n",
       "  'Let us wag, then.',\n",
       "  'Let us about it: it is admirable pleasures and fery honest knaveries.',\n",
       "  'Let us condole the knight; for, lambkins we will live.',\n",
       "  \"Let us on, And publish the occasion of our arms. The commonwealth is sick of their own choice; Their over-greedy love hath surfeited: An habitation giddy and unsure Hath he that buildeth on the vulgar heart. O thou fond many, with what loud applause Didst thou beat heaven with blessing Bolingbroke, Before he was what thou wouldst have him be! And being now trimm'd in thine own desires, Thou, beastly feeder, art so full of him, That thou provokest thyself to cast him up. So, so, thou common dog, didst thou disgorge Thy glutton bosom of the royal Richard; And now thou wouldst eat thy dead vomit up, And howl'st to find it. What trust is in these times? They that, when Richard lived, would have him die, Are now become enamour'd on his grave: Thou, that threw'st dust upon his goodly head When through proud London he came sighing on After the admired heels of Bolingbroke, Criest now 'O earth, yield us that king again, And take thou this!' O thoughts of men accursed! Past and to come seems best; things present worst.\",\n",
       "  'Let us withdraw into the other room.',\n",
       "  \"Let us make the assay upon him: if he care not for't, he will supply us easily; if he covetously reserve it, how shall's get it?\",\n",
       "  'Let us first see peace in Athens: there is no time so miserable but a man may be true.',\n",
       "  'Let us leave here, gentlemen.',\n",
       "  'Let us from it.'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let_us = [x for x in shakespeare if x.startswith('Let us ')]\n",
    "len(let_us), let_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'kill': 1,\n",
       "         'take': 1,\n",
       "         'entreat': 1,\n",
       "         'to': 1,\n",
       "         'withdraw;': 1,\n",
       "         'deal': 1,\n",
       "         'go.': 2,\n",
       "         'score': 1,\n",
       "         'sit': 1,\n",
       "         'not': 1,\n",
       "         'do': 1,\n",
       "         'go': 1,\n",
       "         'from': 2,\n",
       "         'make': 2,\n",
       "         'seek': 1,\n",
       "         'rather': 1,\n",
       "         'hear,': 1,\n",
       "         'confess': 1,\n",
       "         'haste': 1,\n",
       "         'wag,': 1,\n",
       "         'about': 1,\n",
       "         'condole': 1,\n",
       "         'on,': 1,\n",
       "         'withdraw': 1,\n",
       "         'first': 1,\n",
       "         'leave': 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(x.split()[2] for x in let_us)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that $P_{CS}(\"Let\\space us \") = 29 / 31023$ and $P_{CS}(\"make\" | \"Let\\space us \") = 2 / 29$ since we know all the possible samples, it is not possible to do the same for $P_{LW}$ since there might be more cases in the unknown lost works.\n",
    "\n",
    "On that note, observe that $P_{CS}(\"k\" | \"Let\\space us \") = 1 / 29$ since there is only the one \"kill\" starting with 'k', but also $P_{CS}(\"i\" | \"Let\\space us \\space k\") = 1.0$.\n",
    "\n",
    "Since the prefix \"Let us k\" uniquely identifies the whole sample \"Let us kill him, and we'll have corn at our own price. Is't a verdict?\", if we were to encode that sequence, we would only need to encode \"Let us k\", as the rest is fully defined by $P_{CS}$. This is one of those edge cases mentioned near the beginning concerning $P(x) = 0.0$ issues, as in such cases we operate on $X' \\subset X$ rather than $X$ which breaks the math a little, but is not an issue in practice (note above how Huffman code produced codes for symbols with $0.0$ probability)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The takeway for $P(x)$ is that in a lot of practical applications, you just don't know it, but rather you have some samples $s_i$ from it. With those samples you fit some $Q(x)$ so that the samples _could_ have come from it. The issue is that if you assume the samples are all there is and fully represent $P(x)$, $Q(x)$ could diverge significantly for $x'\\in X$ you do not have in your samples, aka overfitting. The information theory wisdom here says that in that case you minimize combined size of produced code (prediction loss) and coding table size (regularization loss, or simply model complexity).\n",
    "\n",
    "Huh, but _why_?\n",
    "\n",
    "Lets recall $D_{KL}(P || Q) = -\\sum_{x\\in X} P(x) log_2 Q(x) - (- \\sum_{x\\in X} P(x) log_2 P(x))$\n",
    "I.E. KL is cross-entropy minus entropy. We don't know $P(x)$, we have a bunch of samples from it though. Lets say $Q(x)$ would generate those samples as well.\n",
    "\n",
    "##### Quick digression\n",
    "Don't get confused by \"generating\" samples here in connection to classification models. In classification as sample is a $s_i = (x_i, y_i)$ pair, you have some other $G(x)$ that generates inputs for classificators, and then you have $P(y_i|x_i)$ and $Q(y_i|x_i)$ as stand-ins for joint distributions $P(x_i, y_i)$ and $Q(x_i, y_i)$. For things like images and other high-dimensonal input we rarely even near the entirety of $x\\in X$ being sampled so $G(x)$ and therefore joint distributions are unknown. $P(y|x)$ is mostly set to $1.0$ for known samples and adjoined label, although there are some cases when it is not so (model distillation, or other uncertainty). It should be noted that this in itself is a mistake in the grand scheme of things, but we don't know any better than to set it to $1.0$ and roll with that.\n",
    "\n",
    "All of the things said so far still stand, if you want to transmit a set of $(image, label)$ pairs, and you have a classifier $Q(label|image)$ that perfectly classifies images you can omit the labels from the coded sequence.\n",
    "\n",
    "###### Mini-quick digression\n",
    "If $x \\in X$ are all possible images what would the distribution of natural images (well, photographs without digital distortions) $G(x)$ look like? Very minor modifications can make an image 'unnatural', a lot of adversial attacks exploit the fact that we don't know when we are given an image outside of the distribution our model should work for.\n",
    "\n",
    "##### Quick digression end\n",
    "\n",
    "Soooo, we were talking about making $Q(x)$ work on input not present in sampled dataset. Since $P(x)$ is what it is, we can only make an impact in $-\\sum_{x\\in X} P(x) log_2 Q(x)$, wherein we assume that $\\delta P(x)$ behaves similar in scale to $\\delta x$ in the vicinity of $x$, in other words $P(x)$ is locally smooth, and simpler models are more locally smooth than complicated ones. This is in the end just hopefull thinking, but it works out well, see also natural gradients for a slightly more elaborate approach to this. Once again, refer to mini-quick digression above, $P(x)$ tends to be more \"sparse\" than we can make $Q(x)$, so when modelling $Q(x)$ we allow it to generate samples \"in between\", even though they would not come from the true distribution.\n",
    "\n",
    "This is a bit of handwaving, but unless you are using probabilistic models and fitting assumed distributions to your data, you are in discriminative black magic territory, and can only mess around with smoothness by changing model complexity, adding regularization, averaging outputs, data augumentation and other, and none of these methods promise anything but often help.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "\n",
    "What is entropy?\n",
    "Most of the time you run into it in practice is while working with a sequence of bytes, and you get something like:\n",
    "$entropy = - \\sum_{i=0 \\to 256} \\frac{count(i)}{N} log_2 \\frac{count(i)}{N}$, with minimum being 0 when there is only one byte value present, and 8 for equally frequent bytes.\n",
    "\n",
    "Well, that is a dirty lie. Observe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def byte_entropy(sequence):\n",
    "    entropy = 0\n",
    "    for sym, count in Counter(sequence).items():\n",
    "        freq = count / len(sequence)\n",
    "        entropy -= freq * math.log2(freq)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the quick brown fox jumps over the lazy dog 4.385453417442482\n",
      "etytoxbjq  a enhmporrol uuz eogfdciv h kws  4.385453417442482\n"
     ]
    }
   ],
   "source": [
    "sequence = 'the quick brown fox jumps over the lazy dog'\n",
    "print(sequence, byte_entropy(sequence))\n",
    "shuffled = np.array([x for x in sequence])\n",
    "np.random.shuffle(shuffled)\n",
    "print(''.join(shuffled), byte_entropy(shuffled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of these is much more 'predictable' than the other, if I tell you \"the quick brown fox\" you can probably guess what the rest of should be __if our samples come from english language distribution__ (you need to know that beforehand). What did we miss?\n",
    "\n",
    "Lets look at $- \\sum_{x\\in X} P(x) log_2 P(x)$\n",
    "\n",
    "Hmm. $- \\sum_{j=1 \\to K} \\sum_{x \\in X} P(x) log_2 P(x)$ lets just look at the sum of needed bits to encode.\n",
    "\n",
    "Uh, oh, wait a moment, we did not make use $j$ anywhere in there. Lets try again.\n",
    "\n",
    "$- \\sum_{j=1 \\to K} \\sum_{x \\in X} P_j(x) log_2 P_j(x)$, better, but what in the world would be $P_j$, that makes even less sense.\n",
    "\n",
    "$- \\sum_{j=1 \\to K} \\sum_{x \\in X} P(x|x_{j-1},\\dots,x_1) log_2 P(x|x_{j-1},\\dots,x_1)$ Better! But this is the number of bits needed to encode some specific sequence, what about a general estimate?\n",
    "\n",
    "$- \\sum_{(x_1, \\dots, x_k), x \\in X, k \\in N} P(x_1,\\dots,x_k) \\sum_{j=1 \\to K} \\sum_{x \\in X} P(x|x_{j-1},\\dots,x_1) log_2 P(x|x_{j-1},\\dots,x_1)$\n",
    "\n",
    "What in the world is that? Well, its an expectation of bits needed to encode/decode a single character over all possible sequences with the code being allowed to know the already decoded characters (for obvious reasons it can't use not yet decoded characters, assuming you do get a sequence of encoded characters, rather then, say, an embedding which is a single information used to seed a decoder).\n",
    "\n",
    "Now, unless we are working with the completed works of Shakespeare, we can't know $P(x_1,\\dots,x_k)$, but given samples we have a pretty decent shot of finding a $Q(x|x_{j-1},\\dots,x_1)$ that is similar to $P(x|x_{j-1},\\dots,x_1)$.\n",
    "\n",
    "Disclaimer: there are many ways to encode stuff, this is just a variant on Huffman code.\n",
    "\n",
    "Thats it, thats the theory, finally done, lets train some models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "tag_counts = Counter('$'.join(shakespeare))\n",
    "tag_map = {tag:i for i, tag in enumerate(sorted(tag_counts.keys()))}\n",
    "tag_map['<start>'] = len(tag_map)\n",
    "tag_map['<pad>'] = len(tag_map)\n",
    "tags = [tag for tag, i in sorted(tag_map.items(), key=lambda x: x[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model. This is set up for batch size 1, I had issues getting it to run with padded sequences, but this works fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# device = torch.device('cuda')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "class ShakespeareRNN(nn.Module):\n",
    "    def __init__(self, num_layers, hidden_size=100, embedding_dim=50):\n",
    "        super(ShakespeareRNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        padding_idx = tag_map['<pad>']\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=len(tags),\n",
    "            embedding_dim=embedding_dim,\n",
    "            padding_idx=padding_idx\n",
    "        )\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.initial_hidden = (nn.Parameter(torch.zeros(num_layers, 1, hidden_size).to(device=device)),\n",
    "                               nn.Parameter(torch.zeros(num_layers, 1, hidden_size).to(device=device)))\n",
    "        \n",
    "        self.output = nn.Linear(hidden_size, len(tags)-2)\n",
    "        \n",
    "    def forward(self, X, X_lenghts, hidden_state=None):        \n",
    "        X = self.embedding(X)\n",
    "        # X = torch.nn.utils.rnn.pack_padded_sequence(X, X_lenghts, batch_first=True)\n",
    "        if hidden_state is None:\n",
    "            outputs, state = self.lstm(X, self.initial_hidden)\n",
    "        else:\n",
    "            outputs, state = self.lstm(X, hidden_state)\n",
    "        # outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "        return self.output(outputs), state    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset and data loader, again, works for batch size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "\n",
    "start_idx = tag_map['<start>']\n",
    "end_idx = tag_map['$']\n",
    "class ShakespeareDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __getitem__(self, x):\n",
    "        return torch.tensor([start_idx] + [tag_map[i] for i in self.data[x]] + [end_idx])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "padding_idx = tag_map['<pad>']\n",
    "def collate_fn(x):\n",
    "    x = sorted(x, key=lambda x: -x.size()[0])\n",
    "    x_padded = torch.nn.utils.rnn.pad_sequence(x, batch_first=True, padding_value=padding_idx)\n",
    "    return x_padded, torch.tensor([len(i) - 1 for i in x])\n",
    "\n",
    "s = ShakespeareDataset([s for s in shakespeare if s])\n",
    "shakespeare_loader = torch.utils.data.DataLoader(\n",
    "    s,\n",
    "    batch_size=1,\n",
    "    num_workers=0,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training with some generation of examples. The pbar use is pretty cool. We train on symbols of a sequence trying to predict the next symbol, thats why we do forward pass on [0, :-1] and run criterion on [0, 1:].\n",
    "\n",
    "Generator temperature parameter helps us to smooth out the output probabilities a bit, so the samples sequences are more varied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "def train(model, optimizer, criterion, epochs=10, lr_decay=0.9, generator_temp=0.8):\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        losses.append([])\n",
    "        pbar = tqdm.tqdm(enumerate(shakespeare_loader), total=len(shakespeare_loader))\n",
    "        loss_avg = 0\n",
    "        model.train()\n",
    "        for i, (X, X_lengths) in pbar:\n",
    "            X_cuda = X.to(device=device)\n",
    "            preds, _ = model(X_cuda[0, :-1].view(1, -1), X_lengths)\n",
    "            loss = 0\n",
    "            for pred, true in zip(preds, X_cuda[0, 1:]):\n",
    "                loss += criterion(pred, true)\n",
    "            loss_avg += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 500 == 0:\n",
    "                losses[-1].append(loss_avg / 500)\n",
    "                pbar.set_description('loss: {:.4f}'.format(loss_avg))\n",
    "                loss_avg = 0\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for _ in range(10):\n",
    "                p = []\n",
    "                predicted = tag_map['<start>']\n",
    "                state = None\n",
    "                for i in range(1000):\n",
    "                    vec = torch.tensor([predicted]).to(device=device)\n",
    "                    len_one = torch.tensor([1])\n",
    "                    probs, state = model.forward(vec.unsqueeze(0), len_one, state)\n",
    "                    predicted = torch.multinomial(probs.data.view(-1).div(generator_temp).exp(), 1)[0]\n",
    "                    if predicted == end_idx:\n",
    "                        break\n",
    "                    p.append(tags[predicted])\n",
    "                print(''.join(p).encode('ascii'))\n",
    "        for group in optimizer.param_groups:\n",
    "            group['lr'] = group['lr'] * 0.9\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training can take quite a while, but there should be a trained model provided with the same result as all seeds are fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ShakespeareRNN(num_layers=3, hidden_size=100, embedding_dim=100).to(device=device)\n",
    "pretrained = True\n",
    "if pretrained:\n",
    "    model.load_state_dict(torch.load('model'))\n",
    "else:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train(model, optimizer, criterion, epochs=10, lr_decay=0.9, generator_temp=0.8)\n",
    "    train(model, optimizer, criterion, epochs=10, lr_decay=0.97, generator_temp=0.8)\n",
    "    torch.save(model.state_dict(), 'model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, you can have some fun playing with the model as an oracle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "def predict(model, string):\n",
    "    encoded = [tag_map['<start>']] + [tag_map[c] for c in string]\n",
    "    probs, state = model.forward(torch.tensor([encoded]).to(device=device),\n",
    "                                 torch.tensor([len(encoded)]))\n",
    "    return probs, state\n",
    "\n",
    "def oracle(model, sequence_init, generator_temp=0.8):\n",
    "    probs, state = predict(model, sequence_init)\n",
    "    p = [c for c in sequence_init]\n",
    "    predicted = torch.multinomial(probs[0][-1].data.view(-1).div(generator_temp).exp(), 1)[0]\n",
    "    p.append(tags[predicted])\n",
    "    for i in range(1000):\n",
    "        vec = torch.tensor([predicted]).to(device=device)\n",
    "        len_one = torch.tensor([1])\n",
    "        probs, state = model.forward(vec.unsqueeze(0), len_one, state)\n",
    "        predicted = torch.multinomial(probs.data.view(-1).div(generator_temp).exp(), 1)[0]\n",
    "        if predicted == end_idx:\n",
    "            break\n",
    "        p.append(tags[predicted])\n",
    "    return ''.join(p).encode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Love is a pierced else avoid the storance.'\n",
      "b'Love tod so.'\n",
      "b'Love the very man.'\n",
      "b'Love speak lord; they see what of the kind of uldess the very hearts are great fatal for the name solenculate.'\n",
      "b\"Love to come with been for all wonderous man for being of the cause; Let again of speak'st to counterform to him.\"\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(oracle(model, 'Love ', 0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we shall combine the Huffman code and probabilities generated by our model. Note that we don't need to encode the sequence end symbol '$'. You can encode him as well, I suppose, if you want to encode a stream of messages I guess. Its easy to include him in the code, but here I'll show how to exclude him. <start> and <pad> were never even outputed by the model.\n",
    "\n",
    "First lets see what we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_tags = tags[0:end_idx] + tags[end_idx + 1:-2]\n",
    "def next_char_code(model, sequence):\n",
    "    probs, _ = predict(model, sequence)\n",
    "    probs = probs.cpu().detach()[0].numpy()\n",
    "    p = probs[-1]\n",
    "    p = np.hstack([p[0:end_idx], p[end_idx + 1:]])\n",
    "    p = np.exp(p) / sum(np.exp(p))\n",
    "    return sorted(huffman(zip(code_tags, p)).items(), key=lambda x: -x[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('W', (0.14264749, '101')),\n",
       " ('I', (0.11243192, '011')),\n",
       " ('A', (0.10398333, '001')),\n",
       " ('T', (0.09462893, '000')),\n",
       " ('H', (0.08030467, '1110')),\n",
       " ('N', (0.07071935, '1100')),\n",
       " ('M', (0.053659864, '0101')),\n",
       " ('S', (0.052735902, '0100')),\n",
       " ('O', (0.04119472, '11011')),\n",
       " ('Y', (0.03658414, '11010')),\n",
       " ('B', (0.03173486, '10010')),\n",
       " ('G', (0.026395047, '10000')),\n",
       " ('C', (0.025057951, '111111')),\n",
       " ('F', (0.02332487, '111110')),\n",
       " ('P', (0.021805987, '111101')),\n",
       " ('D', (0.02079137, '111100')),\n",
       " ('L', (0.017386848, '100111')),\n",
       " (\"'\", (0.013624064, '100010')),\n",
       " ('R', (0.0071174665, '1000111')),\n",
       " ('E', (0.007011211, '1000110')),\n",
       " ('V', (0.0044000708, '10011010')),\n",
       " ('U', (0.00378024, '10011001')),\n",
       " ('K', (0.0028912532, '100110111')),\n",
       " ('J', (0.0011139028, '1001101100')),\n",
       " ('e', (0.0007961873, '1001100001')),\n",
       " ('o', (0.0007092589, '1001100000')),\n",
       " ('Q', (0.00063566153, '10011011010')),\n",
       " ('a', (0.0004057145, '10011000100')),\n",
       " ('.', (0.00028089195, '100110001111')),\n",
       " ('v', (0.0001789701, '1001101101111')),\n",
       " ('p', (0.00017178197, '1001101101110')),\n",
       " ('d', (0.0001388286, '1001100011101')),\n",
       " ('s', (0.00013234087, '1001100011100')),\n",
       " ('l', (0.0001245917, '1001100011010')),\n",
       " ('-', (0.000120002995, '1001100011000')),\n",
       " ('!', (0.00010928732, '1001100010111')),\n",
       " ('u', (0.00010435441, '1001100010101')),\n",
       " (' ', (8.5813896e-05, '10011011011011')),\n",
       " ('t', (7.951254e-05, '10011011011010')),\n",
       " ('k', (6.542301e-05, '10011000110111')),\n",
       " ('j', (6.459405e-05, '10011000110110')),\n",
       " (';', (6.049739e-05, '10011000110010')),\n",
       " ('b', (4.8028603e-05, '10011000101001')),\n",
       " ('m', (3.966309e-05, '100110110110011')),\n",
       " ('i', (3.9352963e-05, '100110110110010')),\n",
       " ('c', (3.8818598e-05, '100110110110001')),\n",
       " ('r', (3.4482375e-05, '100110110110000')),\n",
       " ('f', (3.0921125e-05, '100110001100111')),\n",
       " ('h', (2.8215367e-05, '100110001011011')),\n",
       " ('g', (2.6400583e-05, '100110001011010')),\n",
       " (',', (2.4102785e-05, '100110001011000')),\n",
       " ('q', (2.281849e-05, '100110001010001')),\n",
       " ('?', (2.1355941e-05, '100110001010000')),\n",
       " (':', (1.661346e-05, '1001100011001101')),\n",
       " ('z', (1.3765505e-05, '1001100011001100')),\n",
       " ('n', (1.2943784e-05, '1001100010110010')),\n",
       " ('Z', (9.3085055e-06, '10011000101100111')),\n",
       " ('y', (2.4693375e-06, '100110001011001101')),\n",
       " ('x', (1.0899982e-06, '1001100010110011001')),\n",
       " ('w', (4.4996358e-07, '10011000101100110001')),\n",
       " ('[', (5.0718683e-09, '100110001011001100001')),\n",
       " ('&', (8.0001794e-10, '1001100010110011000000')),\n",
       " ('X', (7.035238e-10, '10011000101100110000011')),\n",
       " (']', (6.358027e-10, '100110001011001100000101')),\n",
       " ('3', (2.3054998e-12, '100110001011001100000100'))]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_char_code(model, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That makes some sense, capital letters are more likely to begin a sequence. Lets try something harder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('d', (0.43711847, '0')),\n",
       " ('t', (0.38024732, '11')),\n",
       " ('c', (0.04595531, '1010')),\n",
       " ('s', (0.03772517, '1000')),\n",
       " (' ', (0.028689615, '10111')),\n",
       " (\"'\", (0.01926543, '10010')),\n",
       " ('k', (0.011997869, '100111')),\n",
       " (',', (0.006857081, '1011001')),\n",
       " ('f', (0.0053545586, '1001101')),\n",
       " ('?', (0.0034888429, '10110101')),\n",
       " ('n', (0.0034238726, '10110100')),\n",
       " ('i', (0.0034004694, '10110001')),\n",
       " ('y', (0.003317221, '10110000')),\n",
       " ('.', (0.002400817, '10011000')),\n",
       " ('!', (0.002331932, '101101111')),\n",
       " ('e', (0.0019245385, '101101101')),\n",
       " ('j', (0.0014458073, '100110011')),\n",
       " ('-', (0.0011579316, '1011011101')),\n",
       " ('l', (0.0009663824, '1011011100')),\n",
       " (';', (0.00049878354, '10110110011')),\n",
       " ('g', (0.00035039728, '10110110000')),\n",
       " (':', (0.0003460674, '10011001011')),\n",
       " ('a', (0.00031722392, '10011001001')),\n",
       " ('u', (0.00022955127, '10011001000')),\n",
       " ('o', (0.00022947072, '101101100101')),\n",
       " ('m', (0.0002288651, '101101100100')),\n",
       " ('r', (0.00019759138, '101101100010')),\n",
       " ('w', (0.00018167713, '100110010101')),\n",
       " ('b', (0.00010805236, '1011011000111')),\n",
       " ('v', (7.8774865e-05, '1001100101001')),\n",
       " ('h', (7.2140916e-05, '1001100101000')),\n",
       " ('p', (5.735723e-05, '10110110001101')),\n",
       " ('q', (1.9155588e-05, '101101100011001')),\n",
       " ('z', (1.6091535e-05, '1011011000110001')),\n",
       " ('x', (1.7444803e-07, '10110110001100001')),\n",
       " ('A', (2.9472275e-11, '101101100011000001')),\n",
       " ('U', (2.211611e-11, '1011011000110000001')),\n",
       " ('N', (8.4693315e-13, '10110110001100000001')),\n",
       " ('S', (2.2791594e-13, '101101100011000000000')),\n",
       " ('X', (1.2253798e-13, '1011011000110000000011')),\n",
       " ('R', (4.8014025e-14, '10110110001100000000100')),\n",
       " ('L', (2.6457867e-14, '101101100011000000001010')),\n",
       " ('[', (2.0697106e-14, '1011011000110000000010110')),\n",
       " ('O', (1.1921402e-14, '10110110001100000000101111')),\n",
       " (']', (1.0219033e-14, '101101100011000000001011101')),\n",
       " ('I', (6.5009387e-16, '1011011000110000000010111001')),\n",
       " ('G', (1.3239542e-16, '10110110001100000000101110001')),\n",
       " ('T', (1.597893e-19, '101101100011000000001011100001')),\n",
       " ('P', (4.1122198e-21, '1011011000110000000010111000001')),\n",
       " ('E', (3.1749993e-21, '10110110001100000000101110000001')),\n",
       " ('H', (1.4704462e-22, '101101100011000000001011100000000')),\n",
       " ('Y', (1.4450338e-22, '1011011000110000000010111000000011')),\n",
       " ('Z', (7.4194375e-23, '10110110001100000000101110000000101')),\n",
       " ('C', (5.714121e-25, '101101100011000000001011100000001001')),\n",
       " ('B', (1.0636882e-25, '1011011000110000000010111000000010001')),\n",
       " ('M', (7.268365e-29, '10110110001100000000101110000000100001')),\n",
       " ('F', (5.767236e-29, '101101100011000000001011100000001000001')),\n",
       " ('3', (1.0817388e-31, '1011011000110000000010111000000010000001')),\n",
       " ('&', (2.3904637e-34, '10110110001100000000101110000000100000001')),\n",
       " ('Q', (6.884259e-35, '101101100011000000001011100000001000000001')),\n",
       " ('W', (6.353167e-35, '1011011000110000000010111000000010000000001')),\n",
       " ('D', (6.4879603e-38, '10110110001100000000101110000000100000000001')),\n",
       " ('J', (3.9664e-41, '101101100011000000001011100000001000000000001')),\n",
       " ('K', (3.967e-42, '1011011000110000000010111000000010000000000001')),\n",
       " ('V', (0.0, '1011011000110000000010111000000010000000000000'))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_char_code(model, 'Begone you foul scoun')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! What else?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Give me a kiss, my lord, or no. Let and order and true.'\n",
      "b'Give me a kiss, my lord.'\n",
      "b'Give me a kiss, my lord.'\n",
      "b\"Give me a kiss, my lord of before thee more as we must be burn'd for it?\"\n",
      "b'Give me a kiss, my lord.'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('l', (0.82327163, '1')),\n",
       " ('g', (0.022836115, '0101')),\n",
       " ('d', (0.018473243, '0011')),\n",
       " ('f', (0.017375054, '0001')),\n",
       " ('n', (0.016868433, '0000')),\n",
       " ('s', (0.01472008, '01110')),\n",
       " ('m', (0.014674191, '01101')),\n",
       " ('r', (0.012373771, '01001')),\n",
       " ('L', (0.010317123, '01000')),\n",
       " ('p', (0.008343307, '011111')),\n",
       " ('b', (0.008193638, '011110')),\n",
       " ('h', (0.007848793, '011001')),\n",
       " ('w', (0.0046810685, '001010')),\n",
       " ('t', (0.004225268, '001000')),\n",
       " ('c', (0.0033100492, '0110001')),\n",
       " ('v', (0.0026347137, '0010111')),\n",
       " ('o', (0.0024251158, '0010110')),\n",
       " ('a', (0.0016070531, '01100001')),\n",
       " ('e', (0.001193854, '00100111')),\n",
       " ('k', (0.0010430771, '00100101')),\n",
       " ('q', (0.00056913967, '001001101')),\n",
       " ('C', (0.0005485768, '001001100')),\n",
       " ('i', (0.0003941601, '0110000010')),\n",
       " (\"'\", (0.0003294751, '0110000001')),\n",
       " ('u', (0.00027932762, '0010010011')),\n",
       " (' ', (0.00025713586, '0010010010')),\n",
       " ('G', (0.00020162125, '01100000110')),\n",
       " ('R', (0.0001413505, '01100000000')),\n",
       " ('H', (0.0001239568, '00100100010')),\n",
       " ('-', (0.0001101784, '00100100000')),\n",
       " ('j', (0.00010240651, '011000001110')),\n",
       " ('S', (9.182322e-05, '011000000011')),\n",
       " ('M', (6.535541e-05, '001001000111')),\n",
       " ('A', (5.8668415e-05, '001001000011')),\n",
       " ('T', (5.8423746e-05, '001001000010')),\n",
       " (',', (4.940008e-05, '0110000011110')),\n",
       " ('P', (3.7303435e-05, '0110000000100')),\n",
       " ('B', (3.1304382e-05, '0010010001100')),\n",
       " ('F', (2.8811734e-05, '01100000111110')),\n",
       " ('?', (2.1563563e-05, '01100000001010')),\n",
       " ('x', (1.7230182e-05, '00100100011011')),\n",
       " ('J', (1.5908954e-05, '00100100011010')),\n",
       " ('E', (1.5250719e-05, '011000001111111')),\n",
       " ('D', (1.4257185e-05, '011000001111110')),\n",
       " ('!', (7.800575e-06, '0110000000101111')),\n",
       " ('y', (5.876156e-06, '0110000000101110')),\n",
       " ('W', (2.645476e-06, '01100000001011011')),\n",
       " ('z', (2.509559e-06, '01100000001011010')),\n",
       " ('N', (2.0445773e-06, '01100000001011001')),\n",
       " ('.', (3.8851297e-07, '011000000010110000')),\n",
       " (';', (2.7507323e-07, '0110000000101100010')),\n",
       " ('I', (1.8137173e-07, '01100000001011000111')),\n",
       " ('V', (4.1427235e-08, '011000000010110001100')),\n",
       " (':', (3.5323048e-08, '0110000000101100011011')),\n",
       " ('O', (1.515125e-08, '01100000001011000110101')),\n",
       " ('X', (1.9696595e-09, '011000000010110001101001')),\n",
       " ('[', (8.189664e-10, '0110000000101100011010001')),\n",
       " ('K', (6.607999e-10, '01100000001011000110100001')),\n",
       " (']', (3.6438772e-11, '011000000010110001101000001')),\n",
       " ('U', (7.2363803e-12, '0110000000101100011010000001')),\n",
       " ('Y', (3.0332908e-13, '01100000001011000110100000001')),\n",
       " ('Q', (8.381191e-21, '011000000010110001101000000001')),\n",
       " ('Z', (1.1501874e-29, '0110000000101100011010000000001')),\n",
       " ('&', (8.454e-42, '01100000001011000110100000000001')),\n",
       " ('3', (0.0, '01100000001011000110100000000000'))]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(oracle(model, 'Give me a kiss, my '))\n",
    "next_char_code(model, 'Give me a kiss, my ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how confident he is here that the next character is 'l'. Lets just sanity check that for a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10465\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(' my lord,', 460),\n",
       " (' my lord.', 286),\n",
       " (' my good', 201),\n",
       " (' my lord', 144),\n",
       " (' my heart', 126),\n",
       " (' my lord;', 115),\n",
       " (' my lord?', 103),\n",
       " (' my father', 101),\n",
       " (' my life,', 75),\n",
       " (\" my father's\", 64),\n",
       " (' my life', 62),\n",
       " (' my Lord', 61),\n",
       " (' my love', 60),\n",
       " (' my troth,', 60),\n",
       " (' my soul', 59),\n",
       " (' my dear', 58),\n",
       " (' my lord:', 57),\n",
       " (' my master', 56),\n",
       " (' my brother', 56),\n",
       " (' my noble', 54),\n",
       " (' my poor', 51),\n",
       " (' my heart,', 51),\n",
       " (' my soul,', 51),\n",
       " (' my lord!', 48),\n",
       " (' my daughter', 47),\n",
       " (' my liege,', 44),\n",
       " (' my lords,', 44),\n",
       " (' my sweet', 41),\n",
       " (' my heart.', 36),\n",
       " (' my father,', 35),\n",
       " (' my lady', 35),\n",
       " (' my son', 34),\n",
       " (' my son,', 34),\n",
       " (' my life.', 34),\n",
       " (' my head', 33),\n",
       " (' my tongue', 33),\n",
       " (' my name', 31),\n",
       " (' my cousin', 31),\n",
       " (' my mother', 30),\n",
       " (' my gracious', 30),\n",
       " (' my wife', 28),\n",
       " (' my love,', 27),\n",
       " (' my old', 26),\n",
       " (' my part,', 26),\n",
       " (' my most', 25),\n",
       " (' my mind', 24),\n",
       " (' my house', 24),\n",
       " (' my young', 24),\n",
       " (' my true', 24),\n",
       " (' my sword', 24),\n",
       " (\" my brother's\", 23),\n",
       " (' my hand,', 23),\n",
       " (' my sister', 23),\n",
       " (' my liege.', 23),\n",
       " (' my very', 22),\n",
       " (' my brother,', 22),\n",
       " (' my friend,', 22),\n",
       " (\" my master's\", 22),\n",
       " (' my master,', 22),\n",
       " (' my mother,', 21),\n",
       " (' my husband', 21),\n",
       " (' my hand', 21),\n",
       " (' my will', 21),\n",
       " (' my mind,', 21),\n",
       " (' my thoughts', 20),\n",
       " (' my gentle', 19),\n",
       " (' my own', 19),\n",
       " (' my wife,', 18),\n",
       " (' my heart;', 18),\n",
       " (' my state', 18),\n",
       " (' my best', 18),\n",
       " (' my face', 17),\n",
       " (' my heart:', 17),\n",
       " (' my friends', 17),\n",
       " (' my heart!', 17),\n",
       " (' my uncle', 17),\n",
       " (' my death', 17),\n",
       " (' my loving', 17),\n",
       " (' my faith,', 17),\n",
       " (' my house,', 17),\n",
       " (' my sword,', 16),\n",
       " (' my blood', 16),\n",
       " (' my leave', 16),\n",
       " (' my love.', 16),\n",
       " (' my friends,', 16),\n",
       " (' my fair', 16),\n",
       " (' my fortunes', 16),\n",
       " (\" my lady's\", 16),\n",
       " (' my conscience,', 16),\n",
       " (' my leave.', 15),\n",
       " (' my blood,', 14),\n",
       " (' my head,', 14),\n",
       " (\" my mother's\", 14),\n",
       " (' my sword.', 14),\n",
       " (' my master.', 14),\n",
       " (' my house.', 14),\n",
       " (' my sovereign', 13),\n",
       " (' my life;', 13),\n",
       " (' my head.', 13),\n",
       " (' my duty', 13),\n",
       " (' my horse,', 13),\n",
       " (' my hand.', 13),\n",
       " (' my father;', 13),\n",
       " (' my boy,', 13),\n",
       " (' my husband,', 12),\n",
       " (' my word,', 12),\n",
       " (' my little', 12),\n",
       " (' my wife.', 12),\n",
       " (' my prayers', 12),\n",
       " (' my part', 12),\n",
       " (' my horse', 12),\n",
       " (' my mind.', 12),\n",
       " (' my knee', 12),\n",
       " (' my body', 12),\n",
       " (' my conscience', 12),\n",
       " (\" my heart's\", 12),\n",
       " (' my royal', 12),\n",
       " (' my will.', 12),\n",
       " (\" my lord's\", 12),\n",
       " (' my name.', 12),\n",
       " (' my sight!', 11),\n",
       " (' my great', 11),\n",
       " (' my service', 11),\n",
       " (' my other', 11),\n",
       " (' my two', 11),\n",
       " (' my tongue,', 11),\n",
       " (' my fortune', 11),\n",
       " (' my power', 11),\n",
       " (' my will,', 11),\n",
       " (' my lips', 11),\n",
       " (' my friend', 11),\n",
       " (\" my mistress'\", 11),\n",
       " (' my father.', 11),\n",
       " (' my former', 11),\n",
       " (' my way', 11),\n",
       " (\" my soul's\", 10),\n",
       " (' my eyes', 10),\n",
       " (' my liege;', 10),\n",
       " (\" my daughter's\", 10),\n",
       " (' my love:', 10),\n",
       " (' my bosom', 10),\n",
       " (' my lord,--', 10),\n",
       " (\" my country's\", 9),\n",
       " (' my time', 9),\n",
       " (' my tongue.', 9),\n",
       " (' my side,', 9),\n",
       " (' my sovereign,', 9),\n",
       " (' my friends.', 9),\n",
       " (' my king', 9),\n",
       " (' my love!', 9),\n",
       " (' my flesh', 9),\n",
       " (' my mistress', 9),\n",
       " (' my child', 9),\n",
       " (' my speech', 9),\n",
       " (' my place', 9),\n",
       " (' my life:', 9),\n",
       " (' my mistress.', 9),\n",
       " (' my mother.', 9),\n",
       " (' my mother:', 8),\n",
       " (' my dearest', 8),\n",
       " (' my breast,', 8),\n",
       " (' my holy', 8),\n",
       " (' my brother:', 8),\n",
       " (' my back,', 8),\n",
       " (' my niece', 8),\n",
       " (' my present', 8),\n",
       " (' my tent', 8),\n",
       " (' my lords.', 8),\n",
       " (\" my husband's\", 8),\n",
       " (' my arms', 8),\n",
       " (' my face,', 8),\n",
       " (' my lady,', 8),\n",
       " (' my state,', 8),\n",
       " (' my brother;', 8),\n",
       " (' my daughter,', 8),\n",
       " (' my consent,', 8),\n",
       " (' my sake,', 8),\n",
       " (' my daughter.', 8),\n",
       " (' my youth', 8),\n",
       " (' my tears', 8),\n",
       " (' my cause', 8),\n",
       " (' my wit', 7),\n",
       " (\" my body's\", 7),\n",
       " (' my flesh,', 7),\n",
       " (' my brave', 7),\n",
       " (' my grave', 7),\n",
       " (' my wretched', 7),\n",
       " (' my hopes', 7),\n",
       " (' my brother.', 7),\n",
       " (' my hands', 7),\n",
       " (' my pretty', 7),\n",
       " (' my tale', 7),\n",
       " (' my soul.', 7),\n",
       " (' my teeth', 7),\n",
       " (' my king,', 7),\n",
       " (' my mouth', 7),\n",
       " (' my death,', 7),\n",
       " (' my hands,', 7),\n",
       " (' my words', 7),\n",
       " (\" my cousin's\", 7),\n",
       " (' my land', 7),\n",
       " (' my soul!', 7),\n",
       " (' my long', 7),\n",
       " (' my house;', 7),\n",
       " (' my daughter?', 7),\n",
       " (' my whole', 7),\n",
       " (' my bones', 7),\n",
       " (' my love?', 7),\n",
       " (' my friend;', 7),\n",
       " (' my lords;', 7),\n",
       " (' my mouth,', 7),\n",
       " (' my side', 7),\n",
       " (' my sister,', 7),\n",
       " (' my fear', 7),\n",
       " (' my lords!', 7),\n",
       " (' my finger', 7),\n",
       " (' my womb,', 6),\n",
       " (' my sword:', 6),\n",
       " (' my way,', 6),\n",
       " (' my country', 6),\n",
       " (' my more', 6),\n",
       " (' my masters,', 6),\n",
       " (' my father?', 6),\n",
       " (' my knee,', 6),\n",
       " (' my lovely', 6),\n",
       " (' my shame', 6),\n",
       " (' my remembrance', 6),\n",
       " (' my kind', 6),\n",
       " (' my servant', 6),\n",
       " (' my presence', 6),\n",
       " (' my kingdom', 6),\n",
       " (' my tender', 6),\n",
       " (' my place,', 6),\n",
       " (' my speech.', 6),\n",
       " (' my company.', 6),\n",
       " (' my sorrows', 6),\n",
       " (' my honour', 6),\n",
       " (' my child,', 6),\n",
       " (' my horse.', 6),\n",
       " (' my free', 6),\n",
       " (' my name,', 6),\n",
       " (' my oath', 6),\n",
       " (' my care', 6),\n",
       " (' my word', 6),\n",
       " (' my grief', 6),\n",
       " (' my suit?', 6),\n",
       " (' my consent', 6),\n",
       " (' my business', 6),\n",
       " (' my father!', 6),\n",
       " (' my right,', 6),\n",
       " (' my queen,', 6),\n",
       " (' my blood.', 6),\n",
       " (' my fault', 6),\n",
       " (' my false', 6),\n",
       " (' my wife?', 6),\n",
       " (' my leave,', 6),\n",
       " (' my face?', 6),\n",
       " (' my daughter;', 6),\n",
       " (' my child.', 6),\n",
       " (' my passion', 6),\n",
       " (' my spirits', 6),\n",
       " (' my tent.', 6),\n",
       " (' my bond.', 6),\n",
       " (' my chamber.', 6),\n",
       " (' my thoughts,', 6),\n",
       " (' my lady.', 6),\n",
       " (' my estate', 6),\n",
       " (' my daughter!', 6),\n",
       " (' my power.', 5),\n",
       " (' my masters!', 5),\n",
       " (' my cheeks,', 5),\n",
       " (\" my love's\", 5),\n",
       " (' my wealth', 5),\n",
       " (' my son!', 5),\n",
       " (' my foot,', 5),\n",
       " (' my peace', 5),\n",
       " (' my death.', 5),\n",
       " (\" my wife's\", 5),\n",
       " (' my God', 5),\n",
       " (' my closet.', 5),\n",
       " (' my grief,', 5),\n",
       " (' my sons', 5),\n",
       " (' my weak', 5),\n",
       " (' my dread', 5),\n",
       " (' my voice', 5),\n",
       " (' my mind;', 5),\n",
       " (' my son?', 5),\n",
       " (' my purse', 5),\n",
       " (' my right', 5),\n",
       " (' my hopes,', 5),\n",
       " (' my fault:', 5),\n",
       " (' my comfort', 5),\n",
       " (' my son.', 5),\n",
       " (' my sad', 5),\n",
       " (' my crown,', 5),\n",
       " (' my sins', 5),\n",
       " (' my end', 5),\n",
       " (' my sight.', 5),\n",
       " (' my knees', 5),\n",
       " (' my breast', 5),\n",
       " (' my hearts!', 5),\n",
       " (' my reputation', 5),\n",
       " (' my chamber,', 5),\n",
       " (' my eye', 5),\n",
       " (' my faith', 5),\n",
       " (' my wife!', 5),\n",
       " (' my letters', 5),\n",
       " (' my lady?', 5),\n",
       " (' my man,', 5),\n",
       " (' my fault,', 5),\n",
       " (' my legs', 5),\n",
       " (' my credit', 5),\n",
       " (' my sick', 5),\n",
       " (' my fortune.', 5),\n",
       " (' my wife;', 5),\n",
       " (' my proper', 5),\n",
       " (' my body,', 5),\n",
       " (' my faith.', 5),\n",
       " (' my daughter:', 5),\n",
       " (' my reason', 5),\n",
       " (' my friend.', 5),\n",
       " (' my mouth.', 5),\n",
       " (' my will;', 5),\n",
       " (' my authority', 5),\n",
       " (' my word.', 5),\n",
       " (' my shame,', 5),\n",
       " (' my countenance', 5),\n",
       " (' my new', 5),\n",
       " (' my house:', 5),\n",
       " (' my thoughts.', 5),\n",
       " (' my sight', 5),\n",
       " (' my Lady', 5),\n",
       " (' my honour,', 5),\n",
       " (' my revenge.', 5),\n",
       " (' my duty,', 5),\n",
       " (' my revenge', 5),\n",
       " (' my love;', 5),\n",
       " (' my full', 5),\n",
       " (' my mistress,', 5),\n",
       " (' my hand;', 5),\n",
       " (' my sake', 5),\n",
       " (' my desires,', 5),\n",
       " (' my suit.', 5),\n",
       " (' my wars', 4),\n",
       " (' my son;', 4),\n",
       " (' my brain,', 4),\n",
       " (' my sworn', 4),\n",
       " (' my speech,', 4),\n",
       " (' my mind!', 4),\n",
       " (' my nature', 4),\n",
       " (' my lips,', 4),\n",
       " (' my business,', 4),\n",
       " (' my master?', 4),\n",
       " (' my soldiers,', 4),\n",
       " (' my guiltless', 4),\n",
       " (' my eyes.', 4),\n",
       " (' my suit', 4),\n",
       " (' my person', 4),\n",
       " (' my pains', 4),\n",
       " (' my legs.', 4),\n",
       " (' my opinion,', 4),\n",
       " (\" my uncle's\", 4),\n",
       " (' my voice,', 4),\n",
       " (' my father:', 4),\n",
       " (' my rest', 4),\n",
       " (' my weary', 4),\n",
       " (' my nails', 4),\n",
       " (' my gage,', 4),\n",
       " (' my blood;', 4),\n",
       " (' my way.', 4),\n",
       " (' my last', 4),\n",
       " (' my grave:', 4),\n",
       " (' my kinsman,', 4),\n",
       " (' my company', 4),\n",
       " (' my knowledge,', 4),\n",
       " (' my followers', 4),\n",
       " (' my face.', 4),\n",
       " (' my crown', 4),\n",
       " (' my knees,', 4),\n",
       " (' my humour', 4),\n",
       " (' my only', 4),\n",
       " (' my man', 4),\n",
       " (' my joy', 4),\n",
       " (\" my man's\", 4),\n",
       " (' my husband?', 4),\n",
       " (' my memory,', 4),\n",
       " (' my office,', 4),\n",
       " (' my foes', 4),\n",
       " (' my brother?', 4),\n",
       " (' my turn', 4),\n",
       " (' my queen.', 4),\n",
       " (' my valiant', 4),\n",
       " (' my mirth,', 4),\n",
       " (' my better', 4),\n",
       " (' my deeds', 4),\n",
       " (' my affection.', 4),\n",
       " (' my letters,', 4),\n",
       " (' my sister;', 4),\n",
       " (\" my queen's\", 4),\n",
       " (' my place:', 4),\n",
       " (' my purpose.', 4),\n",
       " (' my peace.', 4),\n",
       " (' my husband;', 4),\n",
       " (' my name;', 4),\n",
       " (' my master;', 4),\n",
       " (' my foot', 4),\n",
       " (' my trust', 4),\n",
       " (' my head:', 4),\n",
       " (' my modesty,', 4),\n",
       " (' my nose', 4),\n",
       " (' my labour.', 4),\n",
       " (' my purpose', 4),\n",
       " (' my grave.', 4),\n",
       " (' my heels.', 4),\n",
       " (' my lords:', 4),\n",
       " (' my word;', 4),\n",
       " (' my bond;', 4),\n",
       " (' my fool', 4),\n",
       " (' my boy?', 4),\n",
       " (' my cap', 4),\n",
       " (' my business.', 4),\n",
       " (' my hair', 4),\n",
       " (' my senses', 4),\n",
       " (' my breath', 4),\n",
       " (' my brother!', 4),\n",
       " (' my spirit', 4),\n",
       " (' my life!', 4),\n",
       " (' my thanks', 4),\n",
       " (' my promise.', 4),\n",
       " (' my liege:', 4),\n",
       " (' my wit.', 4),\n",
       " (' my study', 4),\n",
       " (' my bed:', 4),\n",
       " (' my sons,', 4),\n",
       " (' my hand:', 4),\n",
       " (' my words,', 4),\n",
       " (' my opinion', 4),\n",
       " (' my gown.', 4),\n",
       " (' my integrity', 4),\n",
       " (' my bed', 4),\n",
       " (' my beauty,', 4),\n",
       " (' my words.', 4),\n",
       " (' my lords', 4),\n",
       " (' my woman', 4),\n",
       " (' my lady:', 4),\n",
       " (' my countrymen,', 3),\n",
       " (' my purpose,', 3),\n",
       " (' my memory', 3),\n",
       " (' my boy', 3),\n",
       " (' my fancy:', 3),\n",
       " (' my desire', 3),\n",
       " (' my lungs', 3),\n",
       " (' my courage', 3),\n",
       " (' my services', 3),\n",
       " (' my name:', 3),\n",
       " (' my request,', 3),\n",
       " (' my friends;', 3),\n",
       " (' my knife', 3),\n",
       " (' my soul:', 3),\n",
       " (' my husband.', 3),\n",
       " (' my woful', 3),\n",
       " (\" my sorrow's\", 3),\n",
       " (' my knowledge.', 3),\n",
       " (' my company,', 3),\n",
       " (' my fortune,', 3),\n",
       " (' my feeble', 3),\n",
       " (' my cousin.', 3),\n",
       " (' my shoulders', 3),\n",
       " (' my service,', 3),\n",
       " (' my book', 3),\n",
       " (' my God,', 3),\n",
       " (' my greatness', 3),\n",
       " (' my glory', 3),\n",
       " (' my wish:', 3),\n",
       " (' my just', 3),\n",
       " (' my fearful', 3),\n",
       " (' my counsel', 3),\n",
       " (' my meaning:', 3),\n",
       " (' my behalf', 3),\n",
       " (' my head;', 3),\n",
       " (' my tent:', 3),\n",
       " (' my tent;', 3),\n",
       " (' my tent,', 3),\n",
       " (' my soldiers', 3),\n",
       " (' my bosom:', 3),\n",
       " (' my cold', 3),\n",
       " (' my shame.', 3),\n",
       " (\" my sovereign's\", 3),\n",
       " (' my companion', 3),\n",
       " (\" my son's\", 3),\n",
       " (' my days', 3),\n",
       " (' my bed,', 3),\n",
       " (' my thoughts:', 3),\n",
       " (' my ring.', 3),\n",
       " (' my birth,', 3),\n",
       " (' my dull', 3),\n",
       " (' my head?', 3),\n",
       " (\" my rapier's\", 3),\n",
       " (' my strong', 3),\n",
       " (' my griefs', 3),\n",
       " (' my sorrow', 3),\n",
       " (' my throne,', 3),\n",
       " (' my son:', 3),\n",
       " (' my boots,', 3),\n",
       " (' my kin,', 3),\n",
       " (' my virtue', 3),\n",
       " (' my thumb,', 3),\n",
       " (' my rapier,', 3),\n",
       " (' my fay,', 3),\n",
       " (' my ghostly', 3),\n",
       " (' my wits', 3),\n",
       " (' my mother!', 3),\n",
       " (' my husband:', 3),\n",
       " (' my closet,', 3),\n",
       " (' my joys', 3),\n",
       " (' my cell', 3),\n",
       " (' my intents', 3),\n",
       " (' my grandsire', 3),\n",
       " (' my soul;', 3),\n",
       " (' my oath,', 3),\n",
       " (' my warlike', 3),\n",
       " (' my tongue:', 3),\n",
       " (' my ewes', 3),\n",
       " (' my humble', 3),\n",
       " (' my suit:', 3),\n",
       " (' my heaven', 3),\n",
       " (' my body;', 3),\n",
       " (' my title', 3),\n",
       " (' my friend?', 3),\n",
       " (' my ears', 3),\n",
       " (' my boy.', 3),\n",
       " (' my commission', 3),\n",
       " (' my brains', 3),\n",
       " (' my dagger', 3),\n",
       " (' my bosom,', 3),\n",
       " (' my negligence,', 3),\n",
       " (' my knowledge', 3),\n",
       " (' my sense.', 3),\n",
       " (' my past', 3),\n",
       " (\" my near'st\", 3),\n",
       " (' my queen', 3),\n",
       " (' my study,', 3),\n",
       " (' my profit', 3),\n",
       " (' my money', 3),\n",
       " (' my desires', 3),\n",
       " (' my fair,', 3),\n",
       " (' my part;', 3),\n",
       " (' my case', 3),\n",
       " (' my suit,', 3),\n",
       " (' my fellow', 3),\n",
       " (' my oath.', 3),\n",
       " (' my liege,--', 3),\n",
       " (' my men', 3),\n",
       " (' my being', 3),\n",
       " (' my girl.', 3),\n",
       " (' my wounded', 3),\n",
       " (' my apparel', 3),\n",
       " (' my wooing', 3),\n",
       " (' my grief.', 3),\n",
       " (' my parentage,', 3),\n",
       " (' my pate', 3),\n",
       " (' my horse!', 3),\n",
       " (' my hope', 3),\n",
       " (' my lodging,', 3),\n",
       " (' my sake.', 3),\n",
       " (' my duty.', 3),\n",
       " (' my princely', 3),\n",
       " (' my people', 3),\n",
       " (' my garments', 3),\n",
       " (' my glass,', 3),\n",
       " (' my pains,', 3),\n",
       " (\" my dear'st\", 3),\n",
       " (' my dog', 3),\n",
       " (' my simple', 3),\n",
       " (' my cousin,', 3),\n",
       " (' my eyelids', 3),\n",
       " (' my gates,', 3),\n",
       " (' my age,', 3),\n",
       " (' my means,', 3),\n",
       " (' my bad', 3),\n",
       " (' my power,', 3),\n",
       " (' my judgment,', 3),\n",
       " (' my throat,', 3),\n",
       " (' my nativity', 3),\n",
       " (' my back', 3),\n",
       " (' my coxcomb.', 3),\n",
       " (' my particular', 3),\n",
       " (' my land,', 3),\n",
       " (' my strength', 3),\n",
       " (' my coming', 3),\n",
       " (' my heart?', 3),\n",
       " (' my brain', 3),\n",
       " (' my fortunes,', 3),\n",
       " (' my purse:', 3),\n",
       " (' my perfect', 3),\n",
       " (' my wife:', 3),\n",
       " (' my neck,', 3),\n",
       " (' my face;', 3),\n",
       " (' my affection,', 3),\n",
       " (' my wars,', 3),\n",
       " (' my chief', 3),\n",
       " (' my sceptre', 3),\n",
       " (' my robe,', 3),\n",
       " (' my eldest', 3),\n",
       " (' my labour', 3),\n",
       " (' my books', 3),\n",
       " (' my sudden', 3),\n",
       " (' my coming,', 3),\n",
       " (' my dog.', 3),\n",
       " (' my patience,', 3),\n",
       " (' my worthless', 3),\n",
       " (' my leg', 3),\n",
       " (' my wish', 3),\n",
       " (' my nature,', 3),\n",
       " (' my respect', 3),\n",
       " (' my acquaintance', 3),\n",
       " (' my maiden', 3),\n",
       " (' my country,', 3),\n",
       " (' my sense', 3),\n",
       " (' my sons;', 3),\n",
       " (' my advice,', 3),\n",
       " (' my judge,', 3),\n",
       " (' my niece,', 3),\n",
       " (' my brothers', 3),\n",
       " (' my breast;', 3),\n",
       " (' my aunt', 3),\n",
       " (' my youth:', 3),\n",
       " (' my lords?', 3),\n",
       " (' my means', 3),\n",
       " (' my report', 3),\n",
       " (' my masters;', 3),\n",
       " (' my valour,', 3),\n",
       " (' my liege!', 3),\n",
       " (' my bloody', 3),\n",
       " (' my king;', 3),\n",
       " (' my story.', 3),\n",
       " (' my thinking,', 3),\n",
       " (' my will:', 3),\n",
       " (' my cause,', 3),\n",
       " (' my brows', 3),\n",
       " (' my friend:', 3),\n",
       " (' my fingers', 3),\n",
       " (' my Nell', 3),\n",
       " (' my weakness', 3),\n",
       " (' my glove', 3),\n",
       " (' my purse.', 3),\n",
       " (' my office', 3),\n",
       " (' my hopes.', 3),\n",
       " (' my manly', 3),\n",
       " (' my deserts', 3),\n",
       " (' my gold', 3),\n",
       " (' my powers,', 3),\n",
       " (' my first', 3),\n",
       " (' my saying', 3),\n",
       " (' my sweet,', 3),\n",
       " (' my shirt.', 3),\n",
       " (' my honest', 3),\n",
       " (' my offence', 3),\n",
       " (' my place.', 3),\n",
       " (' my mistress!', 3),\n",
       " (' my birth', 3),\n",
       " (' my house?', 3),\n",
       " (' my pocket', 3),\n",
       " (' my orchard,', 3),\n",
       " (' my doors', 3),\n",
       " (' my nation?', 3),\n",
       " (' my word:', 3),\n",
       " (' my tears,', 3),\n",
       " (' my stars', 3),\n",
       " (' my prayers;', 2),\n",
       " (' my fellows:', 2),\n",
       " (' my command,', 2),\n",
       " (' my country:', 2),\n",
       " (' my common', 2),\n",
       " (' my view,', 2),\n",
       " (' my breath.', 2),\n",
       " (' my dear,', 2),\n",
       " (' my wounds', 2),\n",
       " (' my praise', 2),\n",
       " (' my throat', 2),\n",
       " (' my misery', 2),\n",
       " (' my sleep,', 2),\n",
       " (' my commission;', 2),\n",
       " (' my money.', 2),\n",
       " (' my person,', 2),\n",
       " (' my success.', 2),\n",
       " (' my beloved', 2),\n",
       " (' my vow', 2),\n",
       " (' my stead,', 2),\n",
       " (' my beating', 2),\n",
       " (' my lawful', 2),\n",
       " (' my shadow', 2),\n",
       " (' my deep', 2),\n",
       " (' my sleep', 2),\n",
       " (' my day,', 2),\n",
       " (' my knee.', 2),\n",
       " (' my angry', 2),\n",
       " (' my body:', 2),\n",
       " (' my advancement', 2),\n",
       " (' my sight?', 2),\n",
       " (' my curse.', 2),\n",
       " (' my naked', 2),\n",
       " (' my charge', 2),\n",
       " (' my meed,', 2),\n",
       " (' my sickly', 2),\n",
       " (' my rage,', 2),\n",
       " (' my feet,', 2),\n",
       " (' my shame;', 2),\n",
       " (' my sovereign;', 2),\n",
       " (' my weapon,', 2),\n",
       " (' my foot-cloth', 2),\n",
       " (' my desert', 2),\n",
       " (' my poverty', 2),\n",
       " (' my greatness,', 2),\n",
       " (' my accursed', 2),\n",
       " (' my brow', 2),\n",
       " (\" my woman's\", 2),\n",
       " (' my thrice', 2),\n",
       " (' my counsel:', 2),\n",
       " (' my bones,', 2),\n",
       " (' my woes', 2),\n",
       " (' my Edward;', 2),\n",
       " (' my loss:', 2),\n",
       " (\" my burthen'd\", 2),\n",
       " (' my children?', 2),\n",
       " (' my hell.', 2),\n",
       " (' my boys', 2),\n",
       " (' my reasons', 2),\n",
       " (' my dangerous', 2),\n",
       " (' my children.', 2),\n",
       " (' my friendship', 2),\n",
       " (' my wrongs:', 2),\n",
       " (' my standard.', 2),\n",
       " (' my armour', 2),\n",
       " (' my trembling', 2),\n",
       " (' my attempt', 2),\n",
       " (' my prince,', 2),\n",
       " (' my greeting', 2),\n",
       " (' my loyalty,', 2),\n",
       " (' my knighthood', 2),\n",
       " (' my shoulder,', 2),\n",
       " (\" my kingdom's\", 2),\n",
       " (' my foe', 2),\n",
       " (' my grieved', 2),\n",
       " (' my shame:', 2),\n",
       " (' my grave,', 2),\n",
       " (' my freedom,', 2),\n",
       " (' my nurse,', 2),\n",
       " (' my inward', 2),\n",
       " (' my answer', 2),\n",
       " (' my meaning', 2),\n",
       " (' my wrongs', 2),\n",
       " (' my rights', 2),\n",
       " (' my claim', 2),\n",
       " (' my English', 2),\n",
       " (' my household', 2),\n",
       " (' my living', 2),\n",
       " (' my name!', 2),\n",
       " (' my liege', 2),\n",
       " (' my want', 2),\n",
       " (' my breast.', 2),\n",
       " (' my knightly', 2),\n",
       " (\" my honour's\", 2),\n",
       " (' my bond', 2),\n",
       " (' my griefs,', 2),\n",
       " (' my glories', 2),\n",
       " (' my sour', 2),\n",
       " (' my sorrow!', 2),\n",
       " (' my condemned', 2),\n",
       " (' my boots:', 2),\n",
       " (' my haste', 2),\n",
       " (' my finger,', 2),\n",
       " (' my thumb', 2),\n",
       " (' my own,', 2),\n",
       " (' my food,', 2),\n",
       " (' my holidame,', 2),\n",
       " (' my wish.', 2),\n",
       " (' my course,', 2),\n",
       " (' my sin', 2),\n",
       " (' my wedding', 2),\n",
       " (' my gossip', 2),\n",
       " (' my kinsmen', 2),\n",
       " (' my soul--', 2),\n",
       " (' my foe.', 2),\n",
       " (' my ancient', 2),\n",
       " (' my weapon', 2),\n",
       " (' my aching', 2),\n",
       " (' my behalf;', 2),\n",
       " (' my conduct', 2),\n",
       " (' my man?', 2),\n",
       " (' my cousin?', 2),\n",
       " (' my bed;', 2),\n",
       " (' my study.', 2),\n",
       " (\" my child's\", 2),\n",
       " (' my soul?', 2),\n",
       " (' my hands:', 2),\n",
       " (' my resolution', 2),\n",
       " (' my veins,', 2),\n",
       " (' my lady!', 2),\n",
       " (' my son-in-law,', 2),\n",
       " (' my heir;', 2),\n",
       " (' my child!', 2),\n",
       " (' my letter,', 2),\n",
       " (' my course.', 2),\n",
       " (' my short', 2),\n",
       " (' my letter', 2),\n",
       " (' my sons.', 2),\n",
       " (' my grief;', 2),\n",
       " (' my kingly', 2),\n",
       " (' my followers.', 2),\n",
       " (' my death!', 2),\n",
       " (' my great-grandfather', 2),\n",
       " (' my wind', 2),\n",
       " (' my request;', 2),\n",
       " (' my daughters', 2),\n",
       " (' my purpose!', 2),\n",
       " (' my cheeks', 2),\n",
       " (' my niece?', 2),\n",
       " (' my hate', 2),\n",
       " (' my constant', 2),\n",
       " (' my hands;', 2),\n",
       " (' my wrath.', 2),\n",
       " (' my sighs', 2),\n",
       " (' my latter', 2),\n",
       " (' my dukedom,', 2),\n",
       " (' my truth,', 2),\n",
       " (' my sea', 2),\n",
       " (' my trespass', 2),\n",
       " (' my faults,', 2),\n",
       " (' my fall,', 2),\n",
       " (' my lands', 2),\n",
       " (' my fear,', 2),\n",
       " (' my stay', 2),\n",
       " (' my matter,', 2),\n",
       " (' my soldier,', 2),\n",
       " (' my serious', 2),\n",
       " (' my regard,', 2),\n",
       " (' my counsel,', 2),\n",
       " (' my parents,', 2),\n",
       " (' my crown;', 2),\n",
       " (' my design,', 2),\n",
       " (' my revenges', 2),\n",
       " (' my bidding.', 2),\n",
       " (' my misery,', 2),\n",
       " (' my jealousies', 2),\n",
       " (' my practise,', 2),\n",
       " (' my lace,', 2),\n",
       " (' my account', 2),\n",
       " (' my revenue', 2),\n",
       " (' my pack', 2),\n",
       " (' my neighbour', 2),\n",
       " (' my white', 2),\n",
       " (' my words:', 2),\n",
       " (' my faith;', 2),\n",
       " (' my turn,', 2),\n",
       " (' my spirit!', 2),\n",
       " (' my success', 2),\n",
       " (' my message.', 2),\n",
       " (' my state:', 2),\n",
       " (' my familiar', 2),\n",
       " (' my affair.', 2),\n",
       " (' my charity', 2),\n",
       " (' my invention,', 2),\n",
       " (' my spirit.', 2),\n",
       " (' my bail.', 2),\n",
       " (' my chance', 2),\n",
       " (' my entreaty', 2),\n",
       " (' my occupation', 2),\n",
       " (' my belly;', 2),\n",
       " (' my bonds', 2),\n",
       " (' my trust,', 2),\n",
       " (' my cousin;', 2),\n",
       " (' my trial', 2),\n",
       " (' my excuse.', 2),\n",
       " (' my dreams', 2),\n",
       " (' my youngest', 2),\n",
       " (' my treasure', 2),\n",
       " (' my hands.', 2),\n",
       " (' my lands,', 2),\n",
       " (' my friend!', 2),\n",
       " (' my farm', 2),\n",
       " (' my choice:', 2),\n",
       " (' my chamber;', 2),\n",
       " (' my bride,', 2),\n",
       " (' my teeth,', 2),\n",
       " (' my shoulder', 2),\n",
       " (' my heel', 2),\n",
       " (' my tale,', 2),\n",
       " (' my custom,', 2),\n",
       " (' my men,', 2),\n",
       " (' my mad', 2),\n",
       " (' my servant?', 2),\n",
       " (' my sense:', 2),\n",
       " (' my office.', 2),\n",
       " (' my wager', 2),\n",
       " (' my state;', 2),\n",
       " (' my books,', 2),\n",
       " (' my dukedom.', 2),\n",
       " (' my slave,', 2),\n",
       " (' my garments.', 2),\n",
       " (' my command.', 2),\n",
       " (' my doublet', 2),\n",
       " (' my comfort.', 2),\n",
       " (' my bottle:', 2),\n",
       " (' my stomach', 2),\n",
       " (' my mean', 2),\n",
       " (' my labours', 2),\n",
       " (' my sinews,', 2),\n",
       " (' my condition', 2),\n",
       " (' my mercy', 2),\n",
       " (' my fellows', 2),\n",
       " (' my instruction', 2),\n",
       " (' my power;', 2),\n",
       " (' my project', 2),\n",
       " (' my nobler', 2),\n",
       " (' my fury', 2),\n",
       " (' my so', 2),\n",
       " (' my court:', 2),\n",
       " (' my bones:', 2),\n",
       " (' my pardon,', 2),\n",
       " (' my displeasure.', 2),\n",
       " (' my kindred.', 2),\n",
       " (' my fortunes:', 2),\n",
       " (' my drift.', 2),\n",
       " (' my cunning', 2),\n",
       " (' my grace.', 2),\n",
       " (' my money,', 2),\n",
       " (' my troth', 2),\n",
       " (' my sake!', 2),\n",
       " (' my child:', 2),\n",
       " (\" my knife's\", 2),\n",
       " (' my sin:', 2),\n",
       " (' my humour.', 2),\n",
       " (' my charge:', 2),\n",
       " (' my peace,', 2),\n",
       " (' my invention', 2),\n",
       " (' my affection', 2),\n",
       " (' my cue', 2),\n",
       " (' my likeness.', 2),\n",
       " (' my fool?', 2),\n",
       " (' my fool,', 2),\n",
       " (' my living,', 2),\n",
       " (' my tongue;', 2),\n",
       " (' my horses;', 2),\n",
       " (' my sister?', 2),\n",
       " (' my nature.', 2),\n",
       " (' my horses', 2),\n",
       " (' my speeches,', 2),\n",
       " (' my messenger.', 2),\n",
       " (' my sister.', 2),\n",
       " (' my curse:', 2),\n",
       " (' my child;', 2),\n",
       " (' my hundred', 2),\n",
       " (' my boy:', 2),\n",
       " (' my wits.', 2),\n",
       " (' my guests:', 2),\n",
       " (' my body.', 2),\n",
       " (' my presence;', 2),\n",
       " (' my outward', 2),\n",
       " (\" my sister's\", 2),\n",
       " (' my beard', 2),\n",
       " (' my fortunes.', 2),\n",
       " (' my lips;', 2),\n",
       " (' my pledge;', 2),\n",
       " (' my exchange:', 2),\n",
       " (' my queen;', 2),\n",
       " (' my liver', 2),\n",
       " (' my children', 2),\n",
       " (' my going,', 2),\n",
       " (' my blood:', 2),\n",
       " (' my bended', 2),\n",
       " (' my performance', 2),\n",
       " (' my news', 2),\n",
       " (' my kingdom,', 2),\n",
       " (' my women!', 2),\n",
       " (' my pillow', 2),\n",
       " (' my dying', 2),\n",
       " (' my fury,', 2),\n",
       " (' my work', 2),\n",
       " (' my fellows,', 2),\n",
       " (' my women,', 2),\n",
       " (' my peril,', 2),\n",
       " (' my good.', 2),\n",
       " (' my coz,', 2),\n",
       " (' my uncle,', 2),\n",
       " (' my pride', 2),\n",
       " (' my fortunes;', 2),\n",
       " (' my tongue?', 2),\n",
       " (' my thigh,', 2),\n",
       " (' my affairs', 2),\n",
       " (' my revenge,', 2),\n",
       " (' my officers', 2),\n",
       " (' my lambs', 2),\n",
       " (' my physic', 2),\n",
       " (' my Rosalind', 2),\n",
       " (' my censure', 2),\n",
       " (' my counterfeiting', 2),\n",
       " (' my Rosalind.', 2),\n",
       " (' my fancy', 2),\n",
       " (' my horns', 2),\n",
       " ...]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "counter = Counter()\n",
    "count = 0\n",
    "reg = re.compile(' my \\S*')\n",
    "for s in shakespeare:\n",
    "    m = reg.findall(s)\n",
    "    count += len(m)\n",
    "    counter.update(m)\n",
    "print(count)\n",
    "sorted(counter.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully that explains the code above a little. The model is overconfident somewhat, but that is a common issue with neural networks: they do not learn distributions as much as find the most likely outcome. Which is good when you need to have a decision, but could cause problems if you expect an actual table of probabilities (as do we for encoding).\n",
    "\n",
    "Lets try to encode stuff. But first some disclaimers. Encoded/compressed stuff usually has two components, the coding table (i.e. model) and the encoded content. The goal of efficient code is to minimize the size of coding table + size of encoded content. That makes our approach problematic since we have a huge model, so I will ignore that componet. Also, when encoding a shorter sequence, if some characters don't appear, they don't even need to have a code assigned, our model here however always predicts for all characters (which could be hacked around, but it does not match a model trained only on a subset of characters). So in short, I will compare our model to a 'fake standard' encoding method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "tags_total = sum(count for tag, count in tag_counts.items() if tag != '$')\n",
    "symbol_freqs = []\n",
    "for tag in tags:\n",
    "    if tag != '$':\n",
    "        symbol_freqs.append((tag, tag_counts[tag] / tags_total))\n",
    "standard_code = huffman(symbol_freqs)\n",
    "inverse_standard_code = {sf[1][1]:sf[0] for sf in standard_code.items()}\n",
    "def standard_encode(sequence):\n",
    "    encoded = []\n",
    "    for s in sequence:\n",
    "        encoded.append(standard_code[s][1])\n",
    "    return ''.join(encoded)\n",
    "def standard_decode(encoded):\n",
    "    start = 0\n",
    "    symbols = []\n",
    "    for end in range(len(encoded) + 1):\n",
    "        decoded = inverse_standard_code.get(encoded[start:end], False)\n",
    "        if decoded:\n",
    "            symbols.append(decoded)\n",
    "            start = end\n",
    "    return ''.join(symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets just test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 11001110111000111100110111100000001111010000010010101110100000110001010110110000000111000100001011001\n",
      "Ye olde test sequence.\n"
     ]
    }
   ],
   "source": [
    "encoded = standard_encode('Ye olde test sequence.')\n",
    "print(len(encoded), encoded)\n",
    "print(standard_decode(encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets make the same for using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_code(p):\n",
    "    p = np.hstack([p[:end_idx], p[end_idx + 1:]])\n",
    "    p = np.exp(p) / sum(np.exp(p))\n",
    "    return huffman(zip(code_tags, p))\n",
    "\n",
    "def shakespeare_encode(model, sequence):\n",
    "    probs, _ = predict(model, sequence)\n",
    "    probs = probs.cpu().detach()[0].numpy()\n",
    "    encoded = []\n",
    "    for s, p in zip(sequence, probs):\n",
    "        code = make_code(p)\n",
    "        encoded.append(code[s][1])\n",
    "    return ''.join(encoded)\n",
    "\n",
    "def shakespeare_decode(model, encoded):\n",
    "    last_tag = start_idx\n",
    "    start = 0\n",
    "    state = None\n",
    "    symbols = []\n",
    "    while start < len(encoded):\n",
    "        vec = torch.tensor([last_tag]).to(device=device)\n",
    "        len_one = torch.tensor([1])\n",
    "        probs, state = model.forward(vec.unsqueeze(0), len_one, state)\n",
    "        p = probs.cpu().detach()[0].numpy()[-1]\n",
    "        code = make_code(p)\n",
    "        inverse_code = {sf[1][1]:sf[0] for sf in code.items()}\n",
    "        sanity = 0\n",
    "        for end in range(start, len(encoded) + 1):\n",
    "            decoded = inverse_code.get(encoded[start:end], False)\n",
    "            if decoded:\n",
    "                symbols.append(decoded)\n",
    "                start = end\n",
    "                last_tag = tag_map[decoded]\n",
    "                sanity = 1\n",
    "                break\n",
    "        if not sanity:\n",
    "            print('Something went terribly wrong', start, encoded[start:], code)\n",
    "            break\n",
    "    return ''.join(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 11010010000011010000111011111000000011001101001111111111100101100111011001100110110\n",
      "Ye olde test sequence.\n"
     ]
    }
   ],
   "source": [
    "encoded = shakespeare_encode(model, 'Ye olde test sequence.')\n",
    "print(len(encoded), encoded)\n",
    "print(shakespeare_decode(model, encoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent! It works! Lets try it out a bit now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(sequence):\n",
    "    sequence = ''.join(s for s in sequence if s in tag_map)\n",
    "    print(sequence)\n",
    "    encoded = standard_encode(sequence)\n",
    "    print('Standard   ', len(encoded), encoded[:100])\n",
    "    assert(sequence == standard_decode(encoded))\n",
    "    encoded = shakespeare_encode(model, sequence)\n",
    "    print('Shakespeare', len(encoded), encoded[:100])\n",
    "    assert(sequence == shakespeare_decode(model, encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love the smell of napalm in the morning.\n",
      "Standard    182 0010101111101111001011011100011110100101000111010000100000101111011111110011000111110011011111000110\n",
      "Shakespeare 121 0111111010011111111011100100000101111111111110101101010111111011011001001101100001010000000011110110\n"
     ]
    }
   ],
   "source": [
    "test('I love the smell of napalm in the morning.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be, that is the question.\n",
      "Standard    177 1100010010011111100110000111100111011111001110011010111101010011111100110000110000111101001010111101\n",
      "Shakespeare 85 0000011111111001011010010011011100010101011011110111101110001111111100000111011111101\n"
     ]
    }
   ],
   "source": [
    "test('To be or not to be, that is the question.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might expect, the more 'Shakespearian' a sequence looks the better it will be encoded. Additionally, the language model has can infer partial words pretty well. It should have an unfair advantage to the standard model as long as the language is similar to english as it uses context to encode better.\n",
    "\n",
    "Lets try a foreign example. This is from the first paragraph on William Shakespeare on German language Wikipedia, modified so it only has mapped characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "William Shakespeare war ein englischer Dramatiker, Lyriker und Schauspieler. Seine Komodien und Tragodien gehoren zu den bedeutendsten Buhnenstucken der Weltliteratur und sind die am haufigsten aufgefuhrten und verfilmten. Das uberlieferte Gesamtwerk umfasst Dramen, epische Versdichtungen sowie Sonette.\n",
      "Standard    1403 0010100011010101111011111010011100100111001010011010101110110101000010011000110000111110110001111011\n",
      "Shakespeare 1724 1010011101111110111111000010101110110110101010010001100000110101001111111010101110001110101000101110\n"
     ]
    }
   ],
   "source": [
    "test('William Shakespeare war ein englischer Dramatiker, Lyriker und Schauspieler.'\n",
    "     ' Seine Komodien und Tragodien gehoren zu den bedeutendsten Buhnenstucken der'\n",
    "     ' Weltliteratur und sind die am haufigsten aufgefuhrten und verfilmten. Das '\n",
    "     'uberlieferte Gesamtwerk umfasst Dramen, epische Versdichtungen sowie Sonette.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here it has worse, as the distribution is sufficiently different from the one the model was trained on.\n",
    "\n",
    "And thats it. Feel free to play around with stuff above and I hope you had fun."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
