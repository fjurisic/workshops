{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import calendar\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "N = 1000000\n",
    "min_time = datetime.date(1970, 1, 1)\n",
    "max_time = datetime.date(2050, 12, 31)\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self):\n",
    "        self.date_pairs = None\n",
    "        self.labels = None\n",
    "        self.datestring_pairs = None\n",
    "        self.locale_pairs = None\n",
    "        self.char_mapping = None\n",
    "\n",
    "dataset = Dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we generate datetime pairs and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_year_perc = 0.6\n",
    "same_year_month_perc = 0.8\n",
    "\n",
    "dataset.date_pairs = []\n",
    "dataset.labels = []\n",
    "\n",
    "for chance, daysdelta in tqdm(zip(np.random.random(N),\n",
    "                                  np.random.randint((max_time - min_time).days, size=N)),\n",
    "                              total=N):\n",
    "    a = min_time + datetime.timedelta(days=int(daysdelta))\n",
    "    if chance > same_year_month_perc:\n",
    "        day = np.random.randint(1, calendar.monthrange(a.year, a.month)[1] + 1)\n",
    "        b = datetime.date(a.year, a.month, day)\n",
    "    elif chance > same_year_perc:\n",
    "        month = np.random.randint(1, 12 + 1)\n",
    "        day = np.random.randint(1, calendar.monthrange(a.year, month)[1] + 1)\n",
    "        b = datetime.date(a.year, month, day)\n",
    "    else:\n",
    "        b = min_time + datetime.timedelta(days=int(np.random.randint((max_time - min_time).days)))   \n",
    "    dataset.date_pairs.append((a, b))\n",
    "    dataset.labels.append(int(a <= b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to date strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import babel\n",
    "from babel.dates import format_date\n",
    "\n",
    "dataset.locale_pairs = np.random.choice(babel.localedata.locale_identifiers(), (N,2))\n",
    "\n",
    "dataset.datestring_pairs = []\n",
    "\n",
    "for (date_a, date_b), (locale_a, locale_b) in tqdm(zip(dataset.date_pairs, dataset.locale_pairs), total=N):\n",
    "    dataset.datestring_pairs.append((format_date(date_a, locale=locale_a),\n",
    "                                     format_date(date_b, locale=locale_b)))\n",
    "dataset.datestring_pairs[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a concise character mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allchars = set()\n",
    "for da, dp in tqdm(dataset.datestring_pairs):\n",
    "    allchars.update([ord(x) for x in da])\n",
    "    allchars.update([ord(x) for x in dp])\n",
    "\n",
    "dataset.char_mapping = np.zeros(len(allchars))\n",
    "for i, char in enumerate(sorted(allchars)):\n",
    "    dataset.char_mapping[i] = char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(dataset, open('dataset.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load generated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self):\n",
    "        self.date_pairs = None\n",
    "        self.labels = None\n",
    "        self.datestring_pairs = None\n",
    "        self.locale_pairs = None\n",
    "        self.char_mapping = None\n",
    "\n",
    "dataset = pickle.load(open('dataset.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets train a model that gets (y, m, d) integer triplets.\n",
    "\n",
    "### Integer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[a.year, a.month, a.day, b.year, b.month, b.day] for a, b in dataset.date_pairs])\n",
    "y = np.array(dataset.labels).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data # this is needed for some reason\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Model\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # Example for nn.Sequential style of layers\n",
    "        self.per_input_layers = nn.Sequential(nn.Linear(3, 20),\n",
    "                                              nn.ReLU(inplace=True),\n",
    "                                              nn.Linear(20, 10),\n",
    "                                              nn.ReLU(inplace=True))\n",
    "        # Example for layers called by hand\n",
    "        self.joint_layer_1 = nn.Linear(20, 100)\n",
    "        # Register parameters yourself\n",
    "        self.joint_layer_2_w = nn.Parameter(torch.randn(1, 100, dtype=torch.float, requires_grad=True))\n",
    "        self.joint_layer_2_b = nn.Parameter(torch.zeros(1, dtype=torch.float, requires_grad=True))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        a, b = x.chunk(2, dim=1)\n",
    "        a, b = self.per_input_layers(a), self.per_input_layers(b)\n",
    "        x = torch.cat((a, b), dim=1)\n",
    "        x = F.relu(self.joint_layer_1(x))\n",
    "        # addmm is compound b + x.mm(w)\n",
    "        x = torch.sigmoid(torch.addmm(self.joint_layer_2_b, x, self.joint_layer_2_w.t()))\n",
    "        return x\n",
    "\n",
    "for p in Net().parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Parameters\n",
    "lr = 0.0001\n",
    "momentum = 0.5\n",
    "batch_size = 1000\n",
    "epochs = 20\n",
    "\n",
    "n_splits = 8\n",
    "kfold = StratifiedKFold(n_splits=n_splits)\n",
    "train_stats = []\n",
    "val_stats = []\n",
    "\n",
    "# All .cuda() calls can be parametrized with .to(device)\n",
    "date_bias = torch.Tensor([1970, 1, 1, 1970, 1, 1]).cuda()\n",
    "date_dev = torch.Tensor([80, 12, 30, 80, 12, 30]).cuda()\n",
    "\n",
    "def train(model, data, optimizer):\n",
    "    model.train() # This enables some training-only effects, such as dropout\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    for i, (x, y) in enumerate(data):\n",
    "        x, y = x.float().cuda(), y.float().cuda()\n",
    "        x = (x - date_bias) / date_dev\n",
    "        output = model(x)\n",
    "        loss = F.binary_cross_entropy(output, y.view_as(output), reduction='sum')\n",
    "        \n",
    "        avg_loss += loss.sum().item()\n",
    "        pred = (output >= 0.5).float()\n",
    "        avg_acc += (y.view_as(pred) == pred).sum().item()\n",
    "        \n",
    "        # This is the quitessential gradient update step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return avg_loss / len(data.dataset), avg_acc / len(data.dataset)\n",
    "\n",
    "def test(model, data):\n",
    "    model.eval() # Opposite of model.train() above\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    with torch.no_grad(): # Disable the operation history tracking needed for gradient computation.\n",
    "        for i, (x, y) in enumerate(data):\n",
    "            x, y = x.float().cuda(), y.float().cuda()\n",
    "            x = (x - date_bias) / date_dev\n",
    "            output = model(x)\n",
    "            loss = F.binary_cross_entropy(output, y.view_as(output), reduction='sum')\n",
    "\n",
    "            avg_loss += loss.sum().item()\n",
    "            pred = (output >= 0.5).float()\n",
    "            avg_acc += (y.view_as(pred) == pred).sum().item()\n",
    "\n",
    "    return avg_loss / len(data.dataset), avg_acc / len(data.dataset)\n",
    "\n",
    "for train_idx, val_idx in kfold.split(X, y):\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "    \n",
    "    # You can also extend torch.utils.data.Dataset, with __getitem__ and __len__ overrides. \n",
    "    # __getitem__ can be slow thanks to DataLoader creating batches with multiple threads\n",
    "    train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                   batch_size=batch_size,\n",
    "                                                   pin_memory=True, num_workers=1,\n",
    "                                                   shuffle=True)\n",
    "    val_dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                                 batch_size=batch_size,\n",
    "                                                 pin_memory=True, num_workers=1)\n",
    "    # Basic linear model should get good performance for this task\n",
    "    # model = nn.Sequential(nn.Linear(6, 1), nn.Sigmoid()).cuda()\n",
    "    model = Net().cuda() \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    tstat = []\n",
    "    vstat = []\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        tstat.append(train(model, train_dataloader, optimizer))\n",
    "        vstat.append(test(model, val_dataloader))\n",
    "    train_stats.append(tstat)\n",
    "    val_stats.append(vstat)\n",
    "    print(vstat[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "f, ax = plt.subplots(1, 2, figsize=(18, 10))\n",
    "for stats in train_stats:\n",
    "    loss = [x[0] for x in stats]\n",
    "    acc = [x[1] for x in stats]\n",
    "    ax[0].plot(np.arange(len(stats)), loss, c='b')\n",
    "    ax[1].plot(np.arange(len(stats)), acc, c='b')\n",
    "for stats in val_stats:\n",
    "    loss = [x[0] for x in stats]\n",
    "    acc = [x[1] for x in stats]\n",
    "    ax[0].plot(np.arange(len(stats)), loss, c='orange')\n",
    "    ax[1].plot(np.arange(len(stats)), acc, c='orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out trained parameters, or full states (which are serialized when saving the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save/Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.torch')\n",
    "\n",
    "model = Net().cuda()\n",
    "model.load_state_dict(torch.load('model.torch'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(torch.from_numpy(X).float().cuda())\n",
    "    preds = (outputs >= 0.5).flatten().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(x, y) for x, y, z in zip(dataset.datestring_pairs, X, preds != y) if z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml]",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
